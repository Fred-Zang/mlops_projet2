
import streamlit as st
#import seaborn as sns
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
import webbrowser

import sys
import re
import spacy
import gensim
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize # pour modele ANN.h5
nltk.download('punkt')                  # pour modele ANN.h5

from PIL import Image

import pickle
import joblib
#from wordcloud import WordCloud

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier

import tensorflow as tf

# --------------------chargement des modÃ¨les--------------------------#

nlp = spacy.load('fr_core_news_md') 
# nlp = pickle.load(open("/airflow/clean_model/nlp_SAV_core_news_md.pickle","rb"))

# load Vectorizer For Gender Prediction                                           /airflow/clean_model/vectoriser_ber
# ---------  chargement des modÃ¨les et dataframes diverses ------------- #
GBC_2_vectorizer = pickle.load(open("/airflow/clean_model/vectoriser_GBC_2-sav_sklearn102","rb"))
                                    # airflow/clean_model/vectoriser_GBC_2-sav_sklearn102

# load pre-trained model
trained = pickle.load(open(r"/airflow/clean_model/trained.pickle","rb"))

# load Model For Prediction
GBC_2_model = pickle.load(open("/airflow/clean_model/GBC_2-sav_sklearn102.pickle","rb"))

SVM_model = pickle.load(open("/airflow/clean_model/SVM_sav-sklearn102.pickle","rb"))


#---------- Chargement des data csv ----------------------------------#


df_origin = pd.read_csv('/airflow/clean_data/reviews_trust.csv')
df = pd.read_csv('/airflow/clean_data/reviews_trust_fr_VF.csv',index_col = 0) 

data = pd.read_csv('/airflow/clean_data/review_trust_fr_lemmantiser_word+2_VF.csv', sep=',', index_col = 0)

# ---------  Fonction d'affichage ------------- #


def insert_img(img):
    col1, col2, col3 = st.columns(3)
    with col1:
        st.write(' ')
    with col2:
        st.image(img)
    with col3:
        st.write(' ')


def insert_head(img1, img2):
    col1, col2, col3, col4, col5 = st.columns(5)
    with col1:
        st.image(img1)
    with col2:
        st.write(' ')
    with col3:
        st.write(' ')
    with col4:
        st.write(' ')
    with col5:
        st.image(img2)



# ---------  Fonction de traitement des corpus ------------- #
@st.cache
def text_tokeniser(my_text):
	nlp = spacy.load('fr_core_news_md')
	docx = nlp(my_text)
	# tokens = [ token.text for token in docx]
	allData = [('"Token":{},\n"Lemma":{}'.format(token.text,token.lemma_))for token in docx ]
	return allData

def text_tokeniser2(my_text):
	nlp = spacy.load('fr_core_news_md')
	docx = nlp(my_text)
	tokens = [ token.text for token in docx]
	
	return tokens

# ---------  Fonction de traitement des corpus ------------- #
# fonction pour transformer un document ( ici une ligne de commentaire) 
# en vecteur Ã  partir des tokens qui le compose
	# entrÃ©e : line_com lignes commentaires Ã  traiter
	#          model prÃ©-entrainÃ©
	# sortie : vecteur reprÃ©sentant le document
def my_doc2vec(doc, trained):
	# dimension de reprÃ©sentation
	p = trained.vectors.shape[1]   # p = 100
	# intit du vecteur
	vec = np.zeros(p)  # array (100,)
	# nbre de tokens trouvÃ©s
	nb=0
	# traitement de chaque token de la ligne de commentaire
	for tk in doc:
	# ne traiter que les tokens reconnus
		try:
			values = trained[tk] # on rÃ©cupÃ¨re le vecteur du token concernÃ©
			vec = vec + values   # on incrÃ©mente ce vecteur dans vec
			nb = nb + 1.0        # le compteur de token d'incrÃ©mente
		except:
			pass  # pour lever l'erreur si aucun token trouvÃ© dans notre modele prÃ©-entrainÃ©
	# moyenne des valeurs uniquement si on a des tokens reconnus
	if (nb > 0.0):
		vec = vec/nb
	return vec  # renvoie le vecteur moyennÃ© ou un vecteur nul si aucun token trouvÃ©

# ---------  Fonction d'affichage DataViz ------------- #
def chart(chart):
	if chart == 'Distribution Star variable cible':
		st.image('/airflow/data_others/JPG-PNG/logo_SatisPy_Project.png')
    		
	elif chart == 'Distribution star avec filtre reponse':
		st.image('/airflow/data_others/JPG-PNG/logo_SatisPy_Project.png')
		if st.checkbox("Afficher l'analyse"):
			st.write("Nous remarquons que les sites marchands donnent peu de rÃ©ponses au clients mÃ©contents ou satisfaits Ã  l'exception des clients trÃ¨s satisfaits, ce qui est anormal et doit Ãªtre signalÃ© au service marketing.:pray:")
			st.write(" ")
			st.markdown("""
			### Ã‰tablissons le classement des notes par Ã©toiles suivant :

			- 1 Ã©toile = client trÃ¨s mÃ©content
			- 2 Ã©toiles = client peu content
			- 3 Ã©toiles = client moyennement satisfait
			- 4 Ã©toiles = client satisfait
			- 5 Ã©toiles = client trÃ¨s satisfait	""")
	elif chart == 'Distribution source avec filtre star':
		st.image('/airflow/data_others/JPG-PNG/logo_SatisPy_Project.png')
		if st.checkbox("Afficher l'analyse"):
			st.markdown("""
			### Variation des donnÃ©es entre ces 2 entreprises

			- TrustedShop a beaucoup + de donnÃ©es 'star' rÃ©coltÃ©es que son concurrent
			- TrustPilot a essentiellement des notes de clients trÃ¨s mÃ©contents
			- TrustedShop a une grande majoritÃ© de clients trÃ¨s satisfaits, jusqu'aux clients peu contents
			""")
    			
	elif chart == 'Distributions des sites marchands de compagny avec filtre star ou filtre source':
		st.image('/airflow/data_others/JPG-PNG/logo_SatisPy_Project.png')
		if st.checkbox("Afficher l'analyse"):
			st.markdown("""
			### Site marchand ***showroom***

			- il a la grande majoritÃ© de notations Ã©toiles
			- il a une grande majoritÃ© de clients trÃ¨s satisfaits et satisfaits, avec cependant un nombre non nÃ©gligeable de clients moyennement satisfaits Ã  trÃ¨s mÃ©contents
			- les sources de notations proviennent des 2 entreprises 'TrustPilot' et 'TrustedShop' avec une grande majoritÃ© pour ce dernier
			""")
			st.markdown("""
			### Site marchand ***VeePee***

			- il a peu de notations comparÃ© Ã  son concurrent 'showRoom'
			- la grande majoritÃ© des notations concerne les clients trÃ¨s mÃ©contents (ğŸ‘‰ voir marketing service )
			- l'entreprise 'VeePee' est la seule source de donnÃ©es de notations
			""")
    			
	elif chart == 'Chronologie des notations par AnnÃ©es':
		st.image('/airflow/data_others/JPG-PNG/logo_SatisPy_Project.png')
		if st.checkbox("Afficher l'analyse"):
			st.markdown("""
			- Il y a 66.34% de NaN sur date_commande, le graphe prÃ©sente essentiellement les informations en 2020 et 2021
			- A partir de 2020, il y a une explosion des commandes, avec une majoritÃ© relative de notations positives et cependant bon nombre de clients trÃ¨s mÃ©contents
			""")
	elif chart == ' ':
		st.write(' ')



# ---------  Fonction de prÃ©paration des donnÃ©es: filter, token, lemm ------------- #

def present_part1(choose_part): 
	
	if choose_part == 'PrÃ©sentation Technique du Filtrage et Stop-Words':
		st.write('### :sparkles: PrÃ©sentation technique ')

		st.write("- L'analyse des sentiment est un problÃ¨me d'analyse des langages naturels. Mais le *'Machine learning'* modÃ¨le n'accepte pas les langages naturels. Il faut numÃ©riser les entrÃ©es (mots) au prÃ©alable.")

		st.write('- Le traitement de langage passe par deux Ã©tapes principales :')

		st.write(':a:) Filtrer les textes')

		st.write("##### 	:dart:Objectif:")
		st.write("Afin d'enlever les informations inutiles et rÃ©duire la taille des donnÃ©es traitÃ©es, un filtrage est nÃ©cessaire sur:")

            
		if st.checkbox("Regex = Traitement des caractÃ¨res ou des mots Ã  conserver ou modifier par une Expression RÃ©guliÃ¨re"):
                
			st.write("Les Expressions rÃ©guliÃ¨res servent Ã  traiter le corpus de texte dans de trÃ¨s nombreux cas :")
			st.write("- transformer tout le corpus en lettres minuscules")
			st.write("- Ã©liminer les caractÃ¨res particuliers comme les chiffres, smileys ou les ponctuations")
			st.write("- faire des recherches de mots, groupes de lettres ou de mots pour les modifier ensuite")
			st.write("- enfin pour de multiples autres applications, si vous voulez + d'info, cliquez sur le lien 'Aide @ Regex'")

			col1, col2, col3, col4, col5 = st.columns(5)
			with col1:
				st.write(' ')
			with col2:
				st.write(' ')
			with col3:
				st.write(' ')
			with col4:
				st.write(' ')
			with col5:
				if st.button('Aide @ Regex'):
					url1 = "https://fr.wikipedia.org/wiki/Expression_r%C3%A9guli%C3%A8re"
					webbrowser.open_new_tab(url1)

 ## ğŸ›‘ğŸ›‘check mots corbeilles ou mot corbeille ? ğŸ›‘ğŸ›‘           
		if st.checkbox("ğŸ“ Mots corbeilles (stop words) = Mots inutiles Ã  Ã©liminer"):
    			
			st.write("Il faut bien enlever les mots corbeilles , car ces mots n'ont pas des sens dans l'analyse. Par exemple: le, la, un, des etc.")
			st.write("Selon le retour d'expÃ©rience de l'anayse du projet, une liste de mots corbeilles personnalisÃ© est fait. (Voir le fiche *liste_no-stop-words_tokens_unique.xlsx*)")

			st.write("Voici notre procÃ©dure pour choix des stop words")
			st.markdown("""
				:one:. Tokenizer les commentaires ğŸ‘‰ 
				:two:. Lemmaniser les tokens ğŸ‘‰
				:three:. Compter des tokens lemmanisÃ©s par frÃ©quences ğŸ‘‰
				:four:. SÃ©parer les tokens/mots qui **apparaissent seulement 1 fois** et que nous choisissons d'Ã©liminer ğŸ‘‰
				:five:. Sauvegarder ces mots corbeille personnalisÃ©s dans un fichier excel ğŸ‘‰ 
				:six:. Enlever les mots corbeille *(agrandissez le document ci-dessous pour observer la frÃ©quence des tokens du dataset)*
                    """)

				# ğŸŒğŸŒğŸŒ@fred: ajouter le jpd dans le meme folder ğŸŒğŸŒğŸŒ
			insert_img('/airflow/data_others/JPG-PNG/sample-image.jpg')  # mots_vides.jpg
		st.write('------------------------------------------------------------------------------')

# Mot particulier
    		#st.write(":paperclip: Mots particulers")
		if st.checkbox("ğŸ“ Mots particulers = Mots importants Ã  traiter"):
    			
			
			st.write("Il faut aussi traiter les mots particuliers dans les donnÃ©es. Dans le projet SatisPy, il y a des mots comme 'ras, rad, ras le bol'. Les mots 'ras' et 'rad' prÃ©sentent 'rien a signaler' ou 'rien Ã  dire', ces sentiments sont est plustÃ´t positifs :smiley:. Cependant le mot 'ras le bol' prÃ©sente un sentiment plustÃ´t nÃ©gatif :angry:")
    			# ğŸŒğŸŒğŸŒ@fred: ajouter le jpd dans le meme folder ğŸŒğŸŒğŸŒ
			 ## ğŸ›‘ğŸ›‘add image apres la recevoir ğŸ›‘ğŸ›‘
			st.image('/airflow/data_others/JPG-PNG/logo_SatisPy_Project.png')
			

			st.write("Notre dÃ©cision : '")
			st.write(":mega: 'ras, r.a.s, rÃ s, r.Ã .s, rad, rÃ d,r.a.d, r.Ã .d, r a s, r Ã  s, r a d, r Ã  d' ğŸ‘‰ sont transformÃ©s en 'bien'")
			st.write(":mega: 'ras le bol' ğŸ‘‰ est transformÃ© en 'mauvais'")

			st.write("ğŸ“œ: il y a aussi des procÃ©dures de **tokenization, Lemmatisation et Vectorisation** Ã  la demande en entrant votre propre message *(Modifiez votre choix en haut de page pour y accÃ©der)*")



	if choose_part == 'Tokeniser, Lemmatiser ou Vectoriser Ã  la demande':
		st.write(':b:) Transformations et Vectorisations')
		st.write('### :sparkles: Comprendre la Tokenisation, Lemmantization et Vectorisation ')
    
		st.write("Tokeniser, Lemmantiser et Vectoriser une phrase en direct, Ã  vous de jouer ! ğŸˆ")
		st.write("- Tokeniser  = Transformer le texte en jetons de mots uniques sous forme de liste")
		st.write("- Lemmatiser = Transformer les mots en leur racine sÃ©mantique ")
		st.write("- Vectoriser = Transformer le texte en matrice de nombres que le modÃ¨le de prÃ©diction pourra utiliser")


		message = st.text_area("Entrer un commentaire puis taper Ctrl + EntrÃ©e pour le valider","Ecrire ici ...") 
        
		if st.button("Tokeniser et Lemmatiser votre message"):
			nlp_result = text_tokeniser(message)
			st.json(nlp_result)
                
		if st.button("Vectoriser votre message"):
			vectorizer = CountVectorizer()
			nlp_result = text_tokeniser2(message)
			nlp_result = [' '.join(nlp_result)]
			text_vec = vectorizer.fit_transform(nlp_result).todense()
			mots = vectorizer.get_feature_names()
			st.write('Message vectorisÃ©:')
			st.write(text_vec)
			df_vec = pd.DataFrame(text_vec, columns = mots)
			st.dataframe(df_vec)
			# test phrase : la livraison est rapide. je suis contente. la service est bon.

		col1, col2, col3 = st.columns(3)
		with col1:
			if st.button('Aide @ Tokenizer'):
				url8 = "https://fr.wikipedia.org/wiki/Tokenisation_(s%C3%A9curit%C3%A9_informatique)"
				webbrowser.open_new_tab(url8)
		with col2:
			if st.button('Aide @ Lemmatizer'):
				url9 ="https://fr.wikipedia.org/wiki/Lemmatisation"
				webbrowser.open_new_tab(url9)
		with col3:                             
			if st.button('Aide @ CountVectorizer'):
				url2 = "https://www.youtube.com/watch?v=FwSD1EM2Qkk"
				webbrowser.open_new_tab(url2)




# ---------  Fonction de prÃ©diction Ã  la demande ------------- #

def present_part(choose_do):
	if choose_do == 'Modelisations Ã  la demande':

		st.markdown('# Modelisations Ã  la demande')
		

		corpus = st.text_area("Entrer un commentaire puis taper Ctrl + EntrÃ©e pour le valider","Ecrire ici ..")
		corpus = corpus.lower()
		
		models = ['GBC_2','SVM', 'ANN']
		model = st.selectbox(label = 'Choix du modÃ¨le', options = models)
		# charger le modele prÃ©entraine widipedia en avance: je ne considÃ¨re pas l'opimisation des temps ici. pas encore, on charge le modÃ¨le meme si le GBC_2 n'utilse pas
		

		if model =='GBC_2':
			pred = GBC_2(corpus)
			sentiment(pred) 
		elif model =='SVM':
			pred = SVM(corpus)
			sentiment(pred)
		elif model == 'ANN':
			ANN = tf.keras.models.load_model(r'/airflow/clean_model/ANN-tensor280.h5')  # anciennement ANN.h5
			pred = prediction(corpus, ANN) 
		#st.write(pred)
			pred = pred[0].tolist()
			pred = pred.index(max(pred))
			sentiment(pred)
			st.write(pred)
		else:
			st.write('')
			# je ne sais pas si nous avons besoin d'ajouter autres modÃ¨le ou pas 		
	
	else:  
		if st.button('Aide @ Regression Logistique'):
			url3 = "https://fr.wikipedia.org/wiki/R%C3%A9gression_logistique"
			webbrowser.open_new_tab(url3)
		if st.button('Aide VidÃ©o Sagesse de la Foule'):
			url4 = "https://youtu.be/7C_YpudYtw8"  # sagesse de la foule Guillaume
			st.video(url4)
		if st.button('Aide @ wikipedia2vec'):
			url5 = "https://wikipedia2vec.github.io/wikipedia2vec/intro/"
			webbrowser.open_new_tab(url5)
		if st.button('Aide tuto video wikipedia2vec + modÃ¨le'):
			url8 = "https://youtu.be/FwSD1EM2Qkk"
			st.video(url8)
		if st.button('Aide vidÃ©o Tuto modÃ¨le prÃ©-entrainÃ© wikipedia2vec'):        
			url6 = "https://www.youtube.com/watch?v=FwSD1EM2Qkk"
			webbrowser.open_new_tab(url6)
		if st.button('Aide VidÃ©o Perceptron'):
			url7 = "https://youtu.be/VlMm4VZ6lk4"  # perceptron
			st.video(url7)


# ---------  Fonction d'affichage de sentime ------------- #
def sentiment(pred):
	if pred == 1:
		st.subheader("PrÃ©diction = Sentiment Positif:smiley: ")
		#st.write('Votre commentaire est classifiÃ©: ', pred)
		#st.write('Merci Ã  votre *positif* commentaire :smile:')
	else:
		st.subheader("PrÃ©diction = Sentiment NÃ©gatif :angry: ")
		#st.write('Votre commentaire est classifiÃ©: ', pred)
		#st.write('Merci Ã  votre *nÃ©gative* commentaire :angry:. Pour amÃ©liorer notre service, la site vous contactera.')

# ---------  Fonction la prÃ©diction par SVM_wiki ------------- #
def SVM(corpus):
	# ğŸŒğŸŒğŸŒ@fred: changer le file path ğŸŒğŸŒğŸŒ
	trained = pickle.load(open('/airflow/clean_model/trained.pickle','rb'))
	# charger le model
	# ğŸŒğŸŒğŸŒ@fred: changer le file path ğŸŒğŸŒğŸŒ
	SVM_wiki = pickle.load(open('/airflow/clean_model/SVM_sav-sklearn102.pickle','rb'))
	text_vec = my_doc2vec(corpus,trained)
	text_vector = pd.DataFrame(my_doc2vec(corpus, trained)).T
	pred = SVM_wiki.predict(text_vector)
	return int(pred)

# ---------  Fonction la prÃ©diction par GBC ------------- #
def GBC_2(corpus):
	# charger le vecteur
	# ğŸŒğŸŒğŸŒ@fred: changer le file path ğŸŒğŸŒğŸŒ
	vectorizer = pickle.load(open('/airflow/clean_model/vectoriser_GBC_2-sav_sklearn102','rb'))
	# charger le model 
	# ğŸŒğŸŒğŸŒ@fred: changer le file path ğŸŒğŸŒğŸŒ
	GBC_2 = pickle.load(open('/airflow/clean_model/GBC_2-sav_sklearn102.pickle','rb'))
	text= pd.Series(corpus)
	text_vec = vectorizer.transform(text).todense()
	#st.write('vectorization for GBC2 ',text_vec)
	pred = GBC_2.predict(text_vec)
	pred = int(pred)
	return pred

# ---------  Fonction la prÃ©diction par ANN, GBC_3 et SVM ------------- #

def prediction(text, model):
	# ğŸŒğŸŒğŸŒ@fred: changer le file path ğŸŒğŸŒğŸŒ
	trained = pickle.load(open('/airflow/clean_model/trained.pickle','rb'))
	my_doc = text.lower()
	my_doc_tk = word_tokenize(my_doc)
	def lemms(corpus_tk):    
		doc = nlp(" ".join(corpus_tk))
		lemm = [token.lemma_ for token in doc]
		return lemm
	my_doc_lem = lemms(my_doc_tk)
	def stop_words_filtering(mots, stop_words) : 
		tokens = []
		for mot in mots:
			if mot not in stop_words: 
				tokens.append(mot)
		return tokens
	# ğŸŒğŸŒğŸŒ@fred: changer le file path ğŸŒğŸŒğŸŒ
	df_stop_word_xls = pd.read_excel('/airflow/clean_data/liste_no-stop-words_tokens_unique.xlsx', header=None)
	update_list_fr = list(df_stop_word_xls[0])
	# initialisation de la variable des mots vides
	stop_words = set()
	stop_words.update(update_list_fr)
	my_doc_sw = stop_words_filtering(my_doc_lem,stop_words)
	my_vec = my_doc2vec(my_doc_sw,trained)
    

	pred_my_doc = model.predict(my_vec.reshape(1,-1))
	return pred_my_doc

# ---------------------- crÃ©er 4 pages diffÃ©rentes sur notre streamlit --------------------------------------  #
# crÃ©er une liste de 4 noms des pages

pages = [
    "Le Projet en DETAILS",
    "Cahier des Charges",
    "Filtrages, Tokenisations, Lemmatisations et Vectorisations",
    "ModÃ©lisations Ã  la demande",
    "Conclusion et REMERciements"]

page = st.sidebar.radio("Aller vers", pages)

# ------------------------------------------------------------------------------------------
# sÃ©paration des pages
# ------------------------------------------------------------------------------------------

if page == pages[0]:  # sur la page 0 Introduction
    # affichage
    st.write("### Frontend Streamlit")
    insert_head(
        'https://datascientest.fr/train/assets/logo_datascientest.png',
        "/airflow/data_others/JPG-PNG/logo_SatisPy_Project.png")

    # title du page
    st.markdown(
        "<h1 style='text-align: center; color: white;'>SatisPy Projet - version MLOps</h1>",
        unsafe_allow_html=True)
    # Ã©crire du texte ( # = taille, ##, ###) ici le titre de la page
    st.write("### PrÃ©sentation GÃ©nÃ©rale !!!:")
    st.write("blablabla....................ğŸ˜‰â€‹")
    st.write("### Rapide aperÃ§u du Projet :")
    st.write(
        "On nous a remis un dataset 'reviews_trust.csv' comportant 19.863 lignes et 11 colonnes qui correspond aux commentaires clients \
        et notation de 1 Ã  5 Ã©toiles sur leurs achats de produit sur 2 sites marchands 'ShowRoom' et 'VeePee'. Ces commentaires proviennent de \
            2 sources rÃ©coltant les avis, 'TrustedShop' et 'TrustPilot' et voici un extrait du dataset :")
    st.write("--- AERER TOUT CA ---")
    st.write("Projet prÃ©sentÃ© et rÃ©alisÃ© par Quan Liu, Ã‰ric Gasniere et Fred Zanghi")


#------------------------------------------------------------------------------------------
# sÃ©paration des pages  
#------------------------------------------------------------------------------------------
elif page==pages[1]:  # sur la page 1 Dataviz
	# affichage
	insert_head('https://datascientest.fr/train/assets/logo_datascientest.png','/airflow/data_others/JPG-PNG/logo_SatisPy_Project.png')
	
	st.markdown("<h2 style='text-align: center; color: white;'>Cahier des Charges</h2>", unsafe_allow_html=True)
	st.markdown("""
	## Objectifï¼š
	- Exploration des donnÃ©es 
	- Visualisation des donnÃ©es
	- PrÃ©paration des donnÃ©es 
	""")

	# afficher les donnÃ©es et la premiÃ¨re analyse

	st.markdown('------------------------------------------------------------------------------------------')
	st.markdown('## ğŸˆExploration des donnÃ©es')
    
	# ğŸŒğŸŒğŸŒ@fred: changer le file path ğŸŒğŸŒğŸŒ

	st.write("## Les donnÃ©es originales")
	st.dataframe(df_origin.sample(60))           # ouf j'ai du changer tous les df en df_origin partout ici !

	if st.checkbox("Afficher les valeurs manquantes"): 
		st.dataframe(df_origin.isna().sum())

	# ğŸ›‘ğŸ›‘je dÃ©place ce bloc de code ici car il est liÃ© avec l'exp des donnÃ©esğŸ›‘ğŸ›‘

	st.markdown('## ğŸˆ Visualisation des donnÃ©es')

	st.markdown("""
	### Objectifï¼š
	- DÃ©finition des mÃ©triques et exigences de performances
	- SchÃ©ma dâ€™implÃ©mentation
	- RÃ©cupÃ©ration de nouvelles donnÃ©es

	""")

	charts = ['Distribution Star variable cible',
	'Distribution star avec filtre reponse',
	'Distribution source avec filtre star', 
	'Distributions des sites marchands de compagny avec filtre star ou filtre source', 
	'Chronologie des notations par AnnÃ©es',' ']

	chart_choose = st.selectbox(label = "Choix de chart", options = charts)
    
	#Afficher les charts
	chart(chart_choose)

	st.markdown('## ğŸˆ PrÃ©paration des donnÃ©es')
	if st.checkbox("Afficher les tÃ¢ches principales"):
		st.markdown("""
		### TÃ¢ches principalesï¼š
		- Nettoyer des donnÃ©es
		- Traitement des donnÃ©es (commentaires, data, star etc)
		- Sauvegarde des donnÃ©es
		""")

	st.markdown(
	"""### Choix de la langue analysÃ©e : Extraire les commentaires en FranÃ§ais
	Il y a plusieurs langues dans les donnÃ©es, afin de modÃ©liser le sentiment.
	Ici le package 'langid' est choisi pour faire la classification des langages. 
	AprÃ¨s la classification, le langage plus frÃ©quent est le FranÃ§ais Ã  87.1 % 
	""")

	#dic_lang = {'Langage' : ['fr','en','it','es','pt','wa','de','oc','mt','nl'],'%':[87.1,4.2,2.1,2.1,1.9,0.6,0.4,0.2,0.2,0.2]}
	#table_lang = pd.DataFrame(dic_lang)
	#fig = plt.figure(figsize = (8,4)) 
	#sns.barplot(x = 'Langage',y = '%', data = table_lang);
	#st.pyplot(fig)
	st.image('/airflow/data_others/JPG-PNG/logo_SatisPy_Project.png')

	st.markdown('------------------------------------------------------------------------------------------')

	st.markdown('## ğŸˆ WordCloud')
	if st.checkbox("Afficher la description du Wordcloud"):
		st.markdown("""
		### WordCloudï¼š
		Le nuage de mots (WordCloud) est une reprÃ©sentation visuelle qui complÃ¨te une section de texte pour aider les lecteurs Ã  mieux visualiser la prÃ©sence de mots clÃ©s du texte concernÃ©
		La bibliothÃ¨que wordcloud implÃ©mente un algorithme permettant d'afficher un nuage de mots d'un texte. Cet algorithme regroupe les Ã©tapes suivantes :
		- Tokeniser le texte passÃ© en paramÃ¨tre
		- Filtrer les mots vides
		- Calculer la frÃ©quence des mots
		- ReprÃ©senter visuellement les mots-clefs les plus frÃ©quents sous forme de nuage de mots (source: https://datascientest.com/)
		""")
 
	# add wordcloud
	st.write('### Wordcloud du dataset original avant traitements et filtrages')
	#â˜» afficher l'image wordcloud
	st.image('/airflow/data_others/JPG-PNG/logo_SatisPy_Project.png')

#------------------------------------------------------------------------------------------
# sÃ©pration des pages  
#------------------------------------------------------------------------------------------
# sur la page 2 Vectorisation & Filtrage 
elif page ==pages[2]:
	
	# affichage
	insert_head('https://datascientest.fr/train/assets/logo_datascientest.png','/airflow/data_others/JPG-PNG/logo_SatisPy_Project.png')
	# title du page
	st.markdown("<h1 style='text-align: center; color: white;'>Filtrages, Tokenisation, Lemmatisations et Vectorisations</h1>", unsafe_allow_html=True)
	st.markdown("""
	### Objectifï¼š
	- Vectorisation & Filtrage des Commentaires
	- DÃ©monstration 
	""")

	st.write("	:bookmark: Le sentiment est classifiÃ© comme 'positif' *1*:smiley: ou 'negatif' *0*:angry:")
	st.write("Voici le tableau entre le sentiment (*1* ou *0*) et les notes correspondantes")
	df = pd.DataFrame({'Sentiment': [0,1],'Note':[[1,2],[3,4,5]]},index = ['nÃ©gatif','positif'])

	st.dataframe(df)
	choose = ['PrÃ©sentation Technique du Filtrage et Stop-Words', 'Tokeniser, Lemmatiser ou Vectoriser Ã  la demande']

	choose_part = st.selectbox(label = 'ğŸˆ CHOISIR ENTRE PRÃ‰SENTATION TECHNIQUE OU DÃ‰MONSTRATION PAR VOUS-MÃŠME ğŸˆ', options  = choose)
	
	present_part1(choose_part)

#------------------------------------------------------------------------------------------
# sÃ©pration des pages  
#------------------------------------------------------------------------------------------
elif page ==pages[3]:  # sur la page 3 Modelisation
    
    #############  TOUS LES MODELES ET DATA ICI SONT RECUPERES DANS /airflow/clean_data ou /airflow/clean_model #############################
    
	# affichage
	insert_head('https://datascientest.fr/train/assets/logo_datascientest.png','/airflow/data_others/JPG-PNG/logo_SatisPy_Project.png')
	
	st.markdown("<h2 style='text-align: center; color: white;'>Nos 3 ModÃ¨les prÃ©sentÃ©s ici</h2>", unsafe_allow_html=True)
	

	# title du page
	st.markdown("<h1 style='text-align: center; color: white;'>ModÃ©lisations Ã  la demande</h1>", unsafe_allow_html=True)

	#st.write("# ModÃ©lisations Ã  la demande")
	message = st.text_area("Entrer un commentaire puis taper Ctrl + EntrÃ©e pour le valider","Ecrire ici ..")  
	corpus = message.lower()
	if st.button("Gradient Boosting Classifier 2"):
		pred = GBC_2(corpus)
# ğŸ›‘ğŸ›‘mÃ©lange anglais et franÃ§ais ??ğŸ›‘ğŸ›‘
		st.success('Your message was classified as {}'.format(pred))
		sentiment = sentiment(pred)
		st.write("Ce modÃ¨le atteint une prÃ©cision de 89% sur les 2 sentiments et donc malgrÃ© tout un taux d'erreurs de 11%")


	if st.button("prÃ©-entrainement avec Wikipedia2vec puis modÃ©lisation par SVM"):
		pred = SVM(corpus)
# ğŸ›‘ğŸ›‘mÃ©lange anglais et franÃ§ais ??ğŸ›‘ğŸ›‘
		st.success('Your message was classified as {}'.format(pred))
		sentiment(pred)
		st.write("Ce modÃ¨le atteint une prÃ©cision de 90% sur les 'satisfaits' et seulement 76% sur les 'mÃ©contents")


	if st.button("ANN RÃ©seaux de Neurones Articiciels"):
		ANN = tf.keras.models.load_model('/airflow/clean_model/ANN-tensor280.h5')
		pred = prediction(corpus, ANN) 
		st.write(pred)
		pred = pred[0].tolist()
		pred = pred.index(max(pred))
		sentiment(pred)
		st.write('')
	st.write("Ce modÃ¨le atteint une prÃ©cision de 86% sur les 'satisfaits' et seulement 73% sur les 'mÃ©contents")
# ğŸ›‘ğŸ›‘ajouter les image dans le dossier et dÃ©commentÃ©ğŸ›‘ğŸ›‘
	#	st.image("sample-image.jpg")  # "ANN-layers_fit_confusion_classifreport.jpg")
	#	#st.write('add img')
	#	st.write("La fonction Loss (coÃ»t des erreurs Ã  minimiser) termine Ã  29.9% ce qui n'est pas optimum ")

#------------------------------------------------------------------------------------------
# sÃ©pration des pages  
#------------------------------------------------------------------------------------------

elif page ==pages[4]:  # sur la page 4 Conclusion
	# affichage
	insert_head('https://datascientest.fr/train/assets/logo_datascientest.png','/airflow/data_others/JPG-PNG/logo_SatisPy_Project.png')
    
	# title du page
	st.markdown("<h1 style='text-align: center; color: white;'>âœ¨ Conclusion âœ¨</h1>", unsafe_allow_html=True)
	st.write("")
	col1, col2, col3 = st.columns([1,3,1])
	with col2:
		st.write("Traiter un sujet comme celui-ci a Ã©tÃ© passionnant et nous aimerions le poursuivre encore")
		st.write("")
		st.write("Nous y avons mis beaucoup de cÅ“ur, Ã  dÃ©couvrir toutes ces notions Ã  travers nos modules de cours et Ã  dÃ©rouler une grande quantitÃ© de modÃ¨les pour trouver des approches et ressources diffÃ©rentes.")
		st.write("")
		st.write("Nous avons le sentiment dâ€™en Ãªtre quâ€™au tout dÃ©but de notre Ã©tude, tant il nous reste de questions Ã  rÃ©soudre, de tests Ã  rÃ©aliser et de nouvelles pistes Ã  explorer.")
		st.write("")
		st.write("Nous espÃ©rons que sa lecture vous a Ã©tÃ© agrÃ©able et vous recommandons de parcourir les vidÃ©os et liens donnÃ©s en annexe si vous souhaitez en savoir plus sur le sujet.")
		st.write("")
		st.markdown("<h1 style='text-align: center; color: white;'>ğŸŒ¼â€‹ Remerciements ğŸŒ¼â€‹</h1>", unsafe_allow_html=True)
		st.write("")
		st.write("Nous tenons Ã  remercier toute lâ€™Ã©quipe de DataScientest pour son Ã©coute et leurs conseils, et particuliÃ¨rement notre mentor de projet Antoine qui nous a suivi chaque semaine en rÃ©union zoom avec un sourire et une patience admirable.")
  
	    